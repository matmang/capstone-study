{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17929a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca291b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.key():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbbcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in parmas.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "877bd092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
    "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5  # 은닉층이 5개\n",
    "activations = {}  # 이곳에 활성화 결과를 저장\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
    "    # w = np.random.randn(node_num, node_num) * 1\n",
    "    w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 활성화 함수도 바꿔가며 실험해보자！\n",
    "    # z = sigmoid(a)\n",
    "    z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 히스토그램 그리기\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21d843ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========iteration:0===========\n",
      "std=0.01:2.302517773623698\n",
      "Xavier:2.3076994167141036\n",
      "He:2.4188390648387577\n",
      "===========iteration:100===========\n",
      "std=0.01:2.30197028583973\n",
      "Xavier:2.2323837579691337\n",
      "He:1.2502227823166723\n",
      "===========iteration:200===========\n",
      "std=0.01:2.302240937429425\n",
      "Xavier:2.081950901107101\n",
      "He:0.6813365860565314\n",
      "===========iteration:300===========\n",
      "std=0.01:2.3032954245608446\n",
      "Xavier:1.7629189734590394\n",
      "He:0.45296803198546637\n",
      "===========iteration:400===========\n",
      "std=0.01:2.2982097072003524\n",
      "Xavier:1.12770353718511\n",
      "He:0.28674831320774463\n",
      "===========iteration:500===========\n",
      "std=0.01:2.3008671193380823\n",
      "Xavier:0.7991171175765035\n",
      "He:0.29410749222962684\n",
      "===========iteration:600===========\n",
      "std=0.01:2.3008594324474605\n",
      "Xavier:0.6718810148353231\n",
      "He:0.30621948211181593\n",
      "===========iteration:700===========\n",
      "std=0.01:2.304095748274936\n",
      "Xavier:0.5562693203029052\n",
      "He:0.3202279539908349\n",
      "===========iteration:800===========\n",
      "std=0.01:2.299771502425367\n",
      "Xavier:0.38631579085235923\n",
      "He:0.22371856842670118\n",
      "===========iteration:900===========\n",
      "std=0.01:2.2948850794998545\n",
      "Xavier:0.33678949974211114\n",
      "He:0.1825506898152384\n",
      "===========iteration:1000===========\n",
      "std=0.01:2.2976144546341972\n",
      "Xavier:0.5578189186053732\n",
      "He:0.3181022810701215\n",
      "===========iteration:1100===========\n",
      "std=0.01:2.3030473860043954\n",
      "Xavier:0.43249921195953017\n",
      "He:0.3265965923146811\n",
      "===========iteration:1200===========\n",
      "std=0.01:2.300067943114474\n",
      "Xavier:0.3080044776516081\n",
      "He:0.2004838810781842\n",
      "===========iteration:1300===========\n",
      "std=0.01:2.300673720348291\n",
      "Xavier:0.4578846137157387\n",
      "He:0.3398953726843583\n",
      "===========iteration:1400===========\n",
      "std=0.01:2.2987166462449107\n",
      "Xavier:0.38088175511738476\n",
      "He:0.2530663994466825\n",
      "===========iteration:1500===========\n",
      "std=0.01:2.2983221107297416\n",
      "Xavier:0.31115114027882496\n",
      "He:0.18474237115369938\n",
      "===========iteration:1600===========\n",
      "std=0.01:2.296955980672723\n",
      "Xavier:0.2838471242910827\n",
      "He:0.22392404385385922\n",
      "===========iteration:1700===========\n",
      "std=0.01:2.300848079578552\n",
      "Xavier:0.4371309755089938\n",
      "He:0.3332262011284393\n",
      "===========iteration:1800===========\n",
      "std=0.01:2.310707548663923\n",
      "Xavier:0.2940039987827399\n",
      "He:0.20097614742351666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m t_train[batch_mask]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m weight_init_types\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 36\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mnetworks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mupdate(networks[key]\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[1;32m     39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m networks[key]\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:142\u001b[0m, in \u001b[0;36mMultiLayerNet.gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"기울기를 구한다(오차역전파법).\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    grads['b1']、grads['b2']、... 각 층의 편향\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[1;32m    145\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:87\u001b[0m, in \u001b[0;36mMultiLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"손실 함수를 구한다.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    손실 함수의 값\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:71\u001b[0m, in \u001b[0;36mMultiLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 71\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from deep_learning_from_scratch_master.ch03.mnist import load_mnist\n",
    "from deep_learning_from_scratch_master.common.util import smooth_curve\n",
    "from deep_learning_from_scratch_master.common.multi_layer_net import MultiLayerNet\n",
    "from deep_learning_from_scratch_master.common.optimizer import SGD\n",
    "\n",
    "\n",
    "# 0. MNIST 데이터 읽기==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "\n",
    "# 1. 실험용 설정==========\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key, weight_type in weight_init_types.items():\n",
    "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "                                  output_size=10, weight_init_std=weight_type)\n",
    "    train_loss[key] = []\n",
    "\n",
    "\n",
    "# 2. 훈련 시작==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in weight_init_types.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizer.update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "# 3. 그래프 그리기==========\n",
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iterations)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e03481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1/16 ==============\n",
      "epoch:0 | 0.117 - 0.082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kkk/Documents/capstone/deep_learning_from_scratch_master/common/functions.py:34: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = x - np.max(x, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | 0.097 - 0.109\n",
      "epoch:2 | 0.097 - 0.131\n",
      "epoch:3 | 0.097 - 0.16\n",
      "epoch:4 | 0.097 - 0.184\n",
      "epoch:5 | 0.097 - 0.204\n",
      "epoch:6 | 0.097 - 0.228\n",
      "epoch:7 | 0.097 - 0.24\n",
      "epoch:8 | 0.097 - 0.256\n",
      "epoch:9 | 0.097 - 0.277\n",
      "epoch:10 | 0.097 - 0.305\n",
      "epoch:11 | 0.097 - 0.319\n",
      "epoch:12 | 0.097 - 0.335\n",
      "epoch:13 | 0.097 - 0.352\n",
      "epoch:14 | 0.097 - 0.357\n",
      "epoch:15 | 0.097 - 0.376\n",
      "epoch:16 | 0.097 - 0.389\n",
      "epoch:17 | 0.097 - 0.415\n",
      "epoch:18 | 0.097 - 0.424\n",
      "epoch:19 | 0.097 - 0.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "/Users/kkk/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net_extend.py:104: RuntimeWarning: overflow encountered in square\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
      "/Users/kkk/opt/anaconda3/envs/scratch/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/kkk/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net_extend.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 2/16 ==============\n",
      "epoch:0 | 0.093 - 0.1\n",
      "epoch:1 | 0.097 - 0.129\n",
      "epoch:2 | 0.097 - 0.159\n",
      "epoch:3 | 0.097 - 0.169\n",
      "epoch:4 | 0.097 - 0.187\n",
      "epoch:5 | 0.097 - 0.212\n",
      "epoch:6 | 0.097 - 0.231\n",
      "epoch:7 | 0.097 - 0.255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weight_scale_list):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============== \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ==============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     train_acc_list, bn_train_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43m__train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(w))\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m__train\u001b[0;34m(weight_init_std)\u001b[0m\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mupdate(_network\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m iter_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 44\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     bn_train_acc \u001b[38;5;241m=\u001b[39m bn_network\u001b[38;5;241m.\u001b[39maccuracy(x_train, t_train)\n\u001b[1;32m     46\u001b[0m     train_acc_list\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net_extend.py:109\u001b[0m, in \u001b[0;36mMultiLayerNetExtend.accuracy\u001b[0;34m(self, X, T)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, T):\n\u001b[0;32m--> 109\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_flg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m T\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m : T \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(T, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net_extend.py:87\u001b[0m, in \u001b[0;36mMultiLayerNetExtend.predict\u001b[0;34m(self, x, train_flg)\u001b[0m\n\u001b[1;32m     85\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x, train_flg)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHwAAABTCAYAAABd5GNdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAALeklEQVR4nO2deXBV1R3HP7+ELGTfCFtICARIo4IQrGARrUtFq6K1VipitSq21trpMlNrp4PT6bS2jp2WmWqxLq3aUWqriFpRaIGACxAMUBYjAUzyCEsWErJv79c/zg15ednuS3IJ8d3PzJt5Z7vnd9/3nuWd+7vniqriEjyEDLcBLmcXV/AgwxU8yHAFDzJcwYMMV/AgwxU8yAh6wUXkZyLyjl/cwV7ilvjFfUNEPhCRBhHZZKOu20WkWETqRWSNiCQNyUkEQNALDuQBl4hIKICIjAfCgNl+cVlWXl+qgD8Aj/VXiYicB6wClgFjgQbgyaE5Bfu4gsMOjMAXWuFLgY1AoV/cIVUt8y2oqhtU9R9Al/heWAq8qap5qloH/AL4mojEDvoMAiDoBVfVFmAbsNCKWghsAbb6xeWJyMMi8tYAqzoP2O1T7yGgBZg+wOMNiKAX3GIzneJeihF8i1/cZlV9TFWvH2AdMUCNX1wN4LbwYSAPWGBNosao6kHgA8zYngScT/fxO1DqgDi/uDigdpDHDQhXcMOHQDxwH/A+gKqexozN9wFlqnpkkHXsA2Z1BERkChABfDrI4waEKzigqo1APvAjTFfewVYrrsfWLSKhIhIJjAJCRCRSRMJ6qebvwA0icqmIRAO/BF5TVbeFDxObgVSMyB1sseLyAETkEb//58uARuApzDjfCPylI1FE6kTkUgBV3Qd8ByP8SczY/YBTJ9Mb4jpABBduCw8yHBNcRJ4TkZMisreXdBGRlSJSJCJ7RGSOU7a4dOJkC/8rsKiP9GuBadZnOWYcdHEYxwRX1TzMWnNvLAZeUMNHQIK1Zu3iIMM5hk8ESn3CHivOxUFGDbcBdhCR5Zhun+jo6Nzs7OxhtujcZufOnRWqOqantOEU/CgwySecZsV1Q1WfBp4GmDt3rubn5ztv3QhGRIp7SxvOLn0tcKc1W58H1KjqsWG0Jyiw1cJF5DXgWeAdVfXaLPMycDmQIiIeYAXmvjOq+mfg38B1QBHGGeDuQI13CRy7XfqTGEFWisirwPOqWthXAVX9Zj/pCnzPZv0uQ4StLt3y7FgKzAE+AzZYvlx393GzwOUcxPYYLiLJwF3AvUAB8EfMBbDeEctcHMHuGP46MAN4EbjBZ3K1WkTcKfMIwu4YvlJVN/aUoKpzh9AeF4ex26XniEhCR0BEEkXkrN/LdRk8dgW/T1WrOwKqegrj+uMywrAreKiISEfActAPd8YkFyexO4avw0zQVlnh+604lxGGXcF/ihH5u1Z4PfCMIxa5OIotwa3l1KdwnRRGPHb/h08DfgPkAJEd8ao6xSG7XBzC7qTteUzrbgO+DLwAvOSUUS7OYVfw0ar6H4xbc7GqPgp81TmzXJzC7qStWURCgIMi8iDGUSHGObNcnMJuC/8BEAU8BOQCdwDfcsooF+foV3BrkeU2Va1TVY+q3q2qt1iepv2VXSQihZbv+cM9pN8lIuUissv63DvA83CxSb9duqq2i8iCQA9sXSh/Aq7GeKTuEJG1qrrfL+tqVX0w0OO7DAy7Y3iBiKwFXgXqOyJV9bU+ynwRKFLVwwAi8grGF91fcJeziF3BI4FK4AqfOAX6Erwnv/OLe8h3i4gsxDwn/UNVLfXP4OumnJ6ebtNkl56wu9LmlIPhm8DLqtosIvcDf6PrRdVRfxc3ZYdsCQrsrrQ9j2nRXVDVb/dRrF+/c1Wt9Ak+A/zOjj0ucLqplZLKBoorGyiuqueGmROYlBTVbzm7XbrvzkWRwM30v1XVDmCaiGRihF4C3O6bQUTG+7hL3QgcsGlP0OD1Kh+XnOK9/Sc4UlHP0VONHK1upKaxtUu+KSkxQye4qv7LN2z5nG/tJXtHmTZrkeZdIBR4TlX3icgvgXxVXQs8JCI3YpZsqzBOkkFPdUMLnxyvZfOn5azdVcbR6kbCR4WQmRzNxMTR5GYkMilpNOlJ0aQnRZGeHEVMhL22O6AdIERkBvC2qmYFXHiQfN4eNWr3KnuP1vDh4Uq2Ha5k/7HTnDjdDEBoiLAgK4WbZk/g6pxx9kUV2dmbr6HdMbyWrmP4ccw9cheblFU3suOzKnaX1lBR10x1Yys1DS0crqintqkNgKzUGL40NYUZ42KZMS6WmWkJJEUPrWOR3S79rG4eN9Jpam1nX1kNBSXVFJRWU1B8irKaJgBGh4UyNi6C+Khw4qPCuX5mHPOmJDN/ajKpsZH9HHnw2G3hNwP/VdUaK5wAXK6qa5wzbeRwqLyOjZ+cZH/ZafYfO83Bk3W0e02HODFhNHMyElmekcjcyUlkj4tlVOjwPcNpd5a+QlVf7wioarWIrADWOGLVCKC2qZW39hzj1fxSPi6pBiA1NoKcCXFckZ3KrEkJzJ6UQGqc8602EOwK3tMlOSI2ExgqVJVD5fVsKjzJpsJyth+poqXdy7TUGB65LpsbZ01kXPy5JW5P2BUtX0R+j7kZAuapz53OmDT8qCona5v59EQtezxmLN5VeoqKuhbATK7unJ/B9bMmMCstHh8P7iGntbUVj8dDU1NTt7TIyEjS0tIIC7P/PKddwb+P2d97NWa2vp7P0aO+qsq+stOs23uc9w9VUHSijtrmtjPpU8ZEc9n0VHIzElk4PYW0xP4XOIYKj8dDbGwskydP7nJhqSqVlZV4PB4yMzNtH8/uLL0e6HY/eyTT1NrO9iNVbDlYzrv7TlBS1UCIwJz0RG6aPZFpY2PIGhPDeRPiiY8avieim5qauokNICIkJydTXl4e0PHsztLXA7d2PG4kIonAK6p6TUC1DTPHa5pYt/cYGw6cZPtnVbS0eQkPDWH+1GQeuHwqV+eMJTkmYrjN7EZvQ8ZAhhK7XXqK/7NlIpIacG3DQFV9C2sKjvLWnrIzs+ms1BiWzctgwbQULs5MIio8eOafds/UKyLpqloCICKT6eHu2blCu1f56HAlL28v4b19J2hp95IzPo6ffGU6i84fT1Zq8Ppf2hX858BWEdkMCGar6OWOWWWXsgJQL4y/kNoWL1sOVrDhwAk2FZZTVd9C/Ogwls5LZ8lF6cwYN3IXC1W1x+57IPdB7E7a1onIXIzIBZgFl8aAaxtivJsfJ6TwbepD4tjalkOJdwxjRyWzMPsursoZyzV1bxBWvxX+h/kAJGTARfeY7++vhIaKrgdNmQ6z7zDf8x6HZr/968deADNvNd83/hra/P4uTcyFnMXm+/oVdOsI0+fDjGuhrQU2/qr7SWVeBllXmnrzHicy/hIqS0NIjo82okfEQUQs2tZKZdlhImuLobAYZvS1rW0ndidt92JcldOAXcA8zGsjunmnnC3W7T3OE0W38IWWTK4M38tlEQe4xluAjLsAWTLbZHp2DRzb1bXgpIs7Bd+zGiqLuqZnXdUpeMFLUHu8a3rO4k7B85/rfkFcuLRT8G2r6Ca4qhHc22ql+xEWbQRvaYBtq0gLfwXPrB9THjcZRCAyASJiwdtGpOcD0nY/AbOX2BYcVe33g2kfkcAuK5yNeX1Df+UWYd7/VQQ83EN6BOa/fRHmVVKT+ztmbm6uqqruLj2ly1/Yoev2HtPm1nZ16QTjb9Dj72d3DG9S1SYRQUQiVPUT6554r9h0U74HOKWqWdZrHn8L3GbHoJlpCaxa5m4vEyh2b9t4rDtka4D1IvIG0Ot+nhZn3JTVvAyuw03Zl8UYx0WAfwJXipPrlC62J203W18fFZGNmFc+9bcDhB035TN51LhE1QDJgN9MymWoCHjFQVU3O2FIX/j6pQN1ItLntp8uZPSW4OQSk53tsTvyeERkFKbnqPTL08Uv3WVwOOl6ccZNWUTCMW7Ka/3yrKXzKdSvY7xqztkVvM8DjrVwteem/CzwoogUYdyUl/R+RJehwH1RXZDhvqguyHAFDzJcwYMMV/AgwxU8yHAFDzJcwYMMV/Ag4/+3Rnd5jeBTKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_from_scratch_master.ch03.mnist import load_mnist\n",
    "from deep_learning_from_scratch_master.common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from deep_learning_from_scratch_master.common.optimizer import SGD, Adam\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 학습 데이터를 줄임\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def __train(weight_init_std):\n",
    "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
    "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
    "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
    "                                weight_init_std=weight_init_std)\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    bn_train_acc_list = []\n",
    "    \n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epoch_cnt = 0\n",
    "    \n",
    "    for i in range(1000000000):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "    \n",
    "        for _network in (bn_network, network):\n",
    "            grads = _network.gradient(x_batch, t_batch)\n",
    "            optimizer.update(_network.params, grads)\n",
    "    \n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            bn_train_acc_list.append(bn_train_acc)\n",
    "    \n",
    "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
    "    \n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_epochs:\n",
    "                break\n",
    "                \n",
    "    return train_acc_list, bn_train_acc_list\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "weight_scale_list = np.logspace(0, -4, num=16)\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "for i, w in enumerate(weight_scale_list):\n",
    "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "    train_acc_list, bn_train_acc_list = __train(w)\n",
    "    \n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.title(\"W:\" + str(w))\n",
    "    if i == 15:\n",
    "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
    "    else:\n",
    "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    if i % 4:\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(\"accuracy\")\n",
    "    if i < 12:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel(\"epochs\")\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a767d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.10333333333333333, test acc:0.1312\n",
      "epoch:1, train acc:0.10666666666666667, test acc:0.1311\n",
      "epoch:2, train acc:0.11333333333333333, test acc:0.1366\n",
      "epoch:3, train acc:0.12333333333333334, test acc:0.1417\n",
      "epoch:4, train acc:0.15333333333333332, test acc:0.1497\n",
      "epoch:5, train acc:0.16666666666666666, test acc:0.1595\n",
      "epoch:6, train acc:0.20333333333333334, test acc:0.1694\n",
      "epoch:7, train acc:0.21, test acc:0.1822\n",
      "epoch:8, train acc:0.21666666666666667, test acc:0.1922\n",
      "epoch:9, train acc:0.24333333333333335, test acc:0.2046\n",
      "epoch:10, train acc:0.24333333333333335, test acc:0.2142\n",
      "epoch:11, train acc:0.25, test acc:0.2279\n",
      "epoch:12, train acc:0.28, test acc:0.2356\n",
      "epoch:13, train acc:0.29333333333333333, test acc:0.244\n",
      "epoch:14, train acc:0.30333333333333334, test acc:0.2543\n",
      "epoch:15, train acc:0.3233333333333333, test acc:0.261\n",
      "epoch:16, train acc:0.3433333333333333, test acc:0.2675\n",
      "epoch:17, train acc:0.35333333333333333, test acc:0.275\n",
      "epoch:18, train acc:0.37666666666666665, test acc:0.2785\n",
      "epoch:19, train acc:0.39, test acc:0.2839\n",
      "epoch:20, train acc:0.3933333333333333, test acc:0.2879\n",
      "epoch:21, train acc:0.39, test acc:0.2909\n",
      "epoch:22, train acc:0.39, test acc:0.2929\n",
      "epoch:23, train acc:0.39666666666666667, test acc:0.2962\n",
      "epoch:24, train acc:0.4066666666666667, test acc:0.3041\n",
      "epoch:25, train acc:0.4033333333333333, test acc:0.2997\n",
      "epoch:26, train acc:0.39666666666666667, test acc:0.3021\n",
      "epoch:27, train acc:0.41333333333333333, test acc:0.3067\n",
      "epoch:28, train acc:0.4, test acc:0.3045\n",
      "epoch:29, train acc:0.39666666666666667, test acc:0.3052\n",
      "epoch:30, train acc:0.39666666666666667, test acc:0.3072\n",
      "epoch:31, train acc:0.4066666666666667, test acc:0.3086\n",
      "epoch:32, train acc:0.42, test acc:0.317\n",
      "epoch:33, train acc:0.43333333333333335, test acc:0.3218\n",
      "epoch:34, train acc:0.43666666666666665, test acc:0.3307\n",
      "epoch:35, train acc:0.43333333333333335, test acc:0.3266\n",
      "epoch:36, train acc:0.43666666666666665, test acc:0.3297\n",
      "epoch:37, train acc:0.43333333333333335, test acc:0.331\n",
      "epoch:38, train acc:0.44, test acc:0.3278\n",
      "epoch:39, train acc:0.46, test acc:0.347\n",
      "epoch:40, train acc:0.4633333333333333, test acc:0.3481\n",
      "epoch:41, train acc:0.48, test acc:0.3593\n",
      "epoch:42, train acc:0.48, test acc:0.3596\n",
      "epoch:43, train acc:0.48333333333333334, test acc:0.3631\n",
      "epoch:44, train acc:0.5, test acc:0.3644\n",
      "epoch:45, train acc:0.49333333333333335, test acc:0.3652\n",
      "epoch:46, train acc:0.49, test acc:0.3619\n",
      "epoch:47, train acc:0.49, test acc:0.3683\n",
      "epoch:48, train acc:0.49666666666666665, test acc:0.3706\n",
      "epoch:49, train acc:0.48333333333333334, test acc:0.3695\n",
      "epoch:50, train acc:0.5033333333333333, test acc:0.3784\n",
      "epoch:51, train acc:0.49666666666666665, test acc:0.3831\n",
      "epoch:52, train acc:0.49, test acc:0.3825\n",
      "epoch:53, train acc:0.5, test acc:0.3895\n",
      "epoch:54, train acc:0.5033333333333333, test acc:0.3956\n",
      "epoch:55, train acc:0.52, test acc:0.4138\n",
      "epoch:56, train acc:0.52, test acc:0.4173\n",
      "epoch:57, train acc:0.5166666666666667, test acc:0.4172\n",
      "epoch:58, train acc:0.53, test acc:0.4229\n",
      "epoch:59, train acc:0.54, test acc:0.4315\n",
      "epoch:60, train acc:0.5333333333333333, test acc:0.4264\n",
      "epoch:61, train acc:0.5466666666666666, test acc:0.4382\n",
      "epoch:62, train acc:0.5533333333333333, test acc:0.4405\n",
      "epoch:63, train acc:0.57, test acc:0.4526\n",
      "epoch:64, train acc:0.58, test acc:0.4548\n",
      "epoch:65, train acc:0.5866666666666667, test acc:0.4592\n",
      "epoch:66, train acc:0.5966666666666667, test acc:0.4737\n",
      "epoch:67, train acc:0.59, test acc:0.4642\n",
      "epoch:68, train acc:0.59, test acc:0.4681\n",
      "epoch:69, train acc:0.5966666666666667, test acc:0.4782\n",
      "epoch:70, train acc:0.6266666666666667, test acc:0.4897\n",
      "epoch:71, train acc:0.6166666666666667, test acc:0.4899\n",
      "epoch:72, train acc:0.6266666666666667, test acc:0.4927\n",
      "epoch:73, train acc:0.6366666666666667, test acc:0.5139\n",
      "epoch:74, train acc:0.6433333333333333, test acc:0.5129\n",
      "epoch:75, train acc:0.6566666666666666, test acc:0.513\n",
      "epoch:76, train acc:0.65, test acc:0.5136\n",
      "epoch:77, train acc:0.6633333333333333, test acc:0.5199\n",
      "epoch:78, train acc:0.67, test acc:0.5281\n",
      "epoch:79, train acc:0.6666666666666666, test acc:0.5268\n",
      "epoch:80, train acc:0.6566666666666666, test acc:0.5194\n",
      "epoch:81, train acc:0.6866666666666666, test acc:0.5257\n",
      "epoch:82, train acc:0.6733333333333333, test acc:0.5413\n",
      "epoch:83, train acc:0.72, test acc:0.5536\n",
      "epoch:84, train acc:0.71, test acc:0.5648\n",
      "epoch:85, train acc:0.6933333333333334, test acc:0.5623\n",
      "epoch:86, train acc:0.7133333333333334, test acc:0.5703\n",
      "epoch:87, train acc:0.7233333333333334, test acc:0.5699\n",
      "epoch:88, train acc:0.71, test acc:0.5613\n",
      "epoch:89, train acc:0.7233333333333334, test acc:0.5692\n",
      "epoch:90, train acc:0.73, test acc:0.5756\n",
      "epoch:91, train acc:0.7366666666666667, test acc:0.5842\n",
      "epoch:92, train acc:0.7266666666666667, test acc:0.5855\n",
      "epoch:93, train acc:0.7466666666666667, test acc:0.5912\n",
      "epoch:94, train acc:0.74, test acc:0.59\n",
      "epoch:95, train acc:0.7533333333333333, test acc:0.5927\n",
      "epoch:96, train acc:0.75, test acc:0.5907\n",
      "epoch:97, train acc:0.7333333333333333, test acc:0.5904\n",
      "epoch:98, train acc:0.7466666666666667, test acc:0.5899\n",
      "epoch:99, train acc:0.7366666666666667, test acc:0.5892\n",
      "epoch:100, train acc:0.7533333333333333, test acc:0.5898\n",
      "epoch:101, train acc:0.74, test acc:0.5889\n",
      "epoch:102, train acc:0.7533333333333333, test acc:0.6001\n",
      "epoch:103, train acc:0.7533333333333333, test acc:0.5971\n",
      "epoch:104, train acc:0.7533333333333333, test acc:0.6032\n",
      "epoch:105, train acc:0.75, test acc:0.6021\n",
      "epoch:106, train acc:0.77, test acc:0.6021\n",
      "epoch:107, train acc:0.77, test acc:0.6051\n",
      "epoch:108, train acc:0.7633333333333333, test acc:0.6088\n",
      "epoch:109, train acc:0.77, test acc:0.615\n",
      "epoch:110, train acc:0.77, test acc:0.6174\n",
      "epoch:111, train acc:0.7666666666666667, test acc:0.613\n",
      "epoch:112, train acc:0.7633333333333333, test acc:0.6096\n",
      "epoch:113, train acc:0.7866666666666666, test acc:0.6127\n",
      "epoch:114, train acc:0.7666666666666667, test acc:0.615\n",
      "epoch:115, train acc:0.7833333333333333, test acc:0.6219\n",
      "epoch:116, train acc:0.78, test acc:0.6205\n",
      "epoch:117, train acc:0.7733333333333333, test acc:0.6194\n",
      "epoch:118, train acc:0.78, test acc:0.6208\n",
      "epoch:119, train acc:0.7833333333333333, test acc:0.6255\n",
      "epoch:120, train acc:0.78, test acc:0.6179\n",
      "epoch:121, train acc:0.7733333333333333, test acc:0.6287\n",
      "epoch:122, train acc:0.7833333333333333, test acc:0.6268\n",
      "epoch:123, train acc:0.7866666666666666, test acc:0.6231\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m iter_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     46\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39maccuracy(x_train, t_train)\n\u001b[0;32m---> 47\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     train_acc_list\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[1;32m     49\u001b[0m     test_acc_list\u001b[38;5;241m.\u001b[39mappend(test_acc)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:97\u001b[0m, in \u001b[0;36mMultiLayerNet.accuracy\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 97\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m : t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(t, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:71\u001b[0m, in \u001b[0;36mMultiLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 71\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/layers.py:13\u001b[0m, in \u001b[0;36mRelu.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_from_scratch_master.ch03.mnist import load_mnist\n",
    "from deep_learning_from_scratch_master.common.multi_layer_net import MultiLayerNet\n",
    "from deep_learning_from_scratch_master.common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（가중치 감쇠） 설정 =======================\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9868f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6549c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3107659209888847\n",
      "=== epoch:1, train acc:0.10333333333333333, test acc:0.1056 ===\n",
      "train loss:2.302594828959014\n",
      "train loss:2.3057162914886766\n",
      "train loss:2.306301363109345\n",
      "=== epoch:2, train acc:0.09666666666666666, test acc:0.1047 ===\n",
      "train loss:2.307870948355259\n",
      "train loss:2.313640405504351\n",
      "train loss:2.3031886075487735\n",
      "=== epoch:3, train acc:0.09333333333333334, test acc:0.1039 ===\n",
      "train loss:2.3027138388871378\n",
      "train loss:2.3068513753861253\n",
      "train loss:2.302238452541912\n",
      "=== epoch:4, train acc:0.09333333333333334, test acc:0.1045 ===\n",
      "train loss:2.286653221796573\n",
      "train loss:2.3000501618207374\n",
      "train loss:2.3095243521750444\n",
      "=== epoch:5, train acc:0.1, test acc:0.1073 ===\n",
      "train loss:2.3073790760986412\n",
      "train loss:2.2967049983027796\n",
      "train loss:2.307687566798303\n",
      "=== epoch:6, train acc:0.10666666666666667, test acc:0.1079 ===\n",
      "train loss:2.3005102810805202\n",
      "train loss:2.289324029232887\n",
      "train loss:2.3030494025664097\n",
      "=== epoch:7, train acc:0.11333333333333333, test acc:0.1087 ===\n",
      "train loss:2.3111669063730957\n",
      "train loss:2.2964584971560336\n",
      "train loss:2.298757259891363\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m network \u001b[38;5;241m=\u001b[39m MultiLayerNetExtend(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, hidden_size_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     22\u001b[0m                               output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, use_dropout\u001b[38;5;241m=\u001b[39muse_dropout, dropout_ration\u001b[38;5;241m=\u001b[39mdropout_ratio)\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     24\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m301\u001b[39m, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     25\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m}, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m train_acc_list, test_acc_list \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_acc_list, trainer\u001b[38;5;241m.\u001b[39mtest_acc_list\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 그래프 그리기==========\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/trainer.py:62\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     x_test_sample, t_test_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test[:t], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test[:t]\n\u001b[1;32m     61\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(x_train_sample, t_train_sample)\n\u001b[0;32m---> 62\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_test_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_acc_list\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_acc_list\u001b[38;5;241m.\u001b[39mappend(test_acc)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net_extend.py:109\u001b[0m, in \u001b[0;36mMultiLayerNetExtend.accuracy\u001b[0;34m(self, X, T)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, T):\n\u001b[0;32m--> 109\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_flg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m T\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m : T \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(T, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net_extend.py:87\u001b[0m, in \u001b[0;36mMultiLayerNetExtend.predict\u001b[0;34m(self, x, train_flg)\u001b[0m\n\u001b[1;32m     85\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x, train_flg)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_from_scratch_master.ch03.mnist import load_mnist\n",
    "from deep_learning_from_scratch_master.common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from deep_learning_from_scratch_master.common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda3eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(x):\n",
    "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용 \"\"\"\n",
    "    \n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"데이터셋을 뒤섞는다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 훈련 데이터\n",
    "    t : 정답 레이블\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
    "    return (input_size + 2*pad - filter_size) / stride + 1\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6924a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_trian, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "validation_rate = 0.29\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff49dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.08 | lr:4.4052582728949077e-05, weight decay:6.848218354484552e-06\n",
      "val acc:0.18 | lr:4.6848950283757326e-05, weight decay:4.7268278062439774e-07\n",
      "val acc:0.04 | lr:7.788107811359954e-05, weight decay:1.8822213237531261e-06\n",
      "val acc:0.14 | lr:0.0007408831203964518, weight decay:1.184380790792784e-07\n",
      "val acc:0.13 | lr:0.0006398778125863887, weight decay:3.6605169389974267e-07\n",
      "val acc:0.09 | lr:4.2510668342184985e-06, weight decay:1.1152362306255432e-07\n",
      "val acc:0.15 | lr:1.0481737000355649e-06, weight decay:5.733961559933544e-07\n",
      "val acc:0.15 | lr:6.227642151737329e-05, weight decay:2.7767077211203177e-08\n",
      "val acc:0.09 | lr:1.752235255081167e-06, weight decay:2.886636458160181e-08\n",
      "val acc:0.13 | lr:1.4645394649446274e-06, weight decay:1.4681637716434112e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# ================================================\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m val_acc_list, train_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43m__train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval acc:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val_acc_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | lr:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, weight decay:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(weight_decay))\n\u001b[1;32m     49\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, weight decay:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(weight_decay)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m__train\u001b[0;34m(lr, weight_decay, epocs)\u001b[0m\n\u001b[1;32m     27\u001b[0m network \u001b[38;5;241m=\u001b[39m MultiLayerNet(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, hidden_size_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     28\u001b[0m                         output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_decay_lambda\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_val, t_val,\n\u001b[1;32m     30\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mepocs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     31\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr}, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtest_acc_list, trainer\u001b[38;5;241m.\u001b[39mtrain_acc_list\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/trainer.py:62\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     x_test_sample, t_test_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test[:t], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test[:t]\n\u001b[1;32m     61\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(x_train_sample, t_train_sample)\n\u001b[0;32m---> 62\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_test_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_acc_list\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_acc_list\u001b[38;5;241m.\u001b[39mappend(test_acc)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:97\u001b[0m, in \u001b[0;36mMultiLayerNet.accuracy\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 97\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m : t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(t, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/capstone/deep_learning_from_scratch_master/common/multi_layer_net.py:71\u001b[0m, in \u001b[0;36mMultiLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 71\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_from_scratch_master.ch03.mnist import load_mnist\n",
    "from deep_learning_from_scratch_master.common.multi_layer_net import MultiLayerNet\n",
    "from deep_learning_from_scratch_master.common.util import shuffle_dataset\n",
    "from deep_learning_from_scratch_master.common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 무작위 탐색======================================\n",
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 하이퍼파라미터의 범위 지정===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "\n",
    "# 그래프 그리기========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfc931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
